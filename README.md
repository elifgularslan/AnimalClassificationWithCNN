#  Animals-10 Image Classification with  CNN
## ğŸ¯ Project Goal
The primary objective of this project is to develop a robust Deep Learning model capable of classifying images of 10 different animal species. Instead of using pre-trained models (Transfer Learning), a **Custom Convolutional Neural Network (CNN)** was built from scratch.

---

## ğŸ“‚ Dataset Details
The model was trained on the **Animals-10** dataset. The dataset contains **26,179 images** in total.

* **Training Set:** 20,944 images
* **Validation Set:** 5,235 images
* **Classes:** The dataset includes 10 classes with labels originally in Italian:

| Class Label | English Translation |
|:---:|:---:|
| **Cane** | Dog |
| **Cavallo** | Horse |
| **Elefante** | Elephant |
| **Farfalla** | Butterfly |
| **Gallina** | Chicken |
| **Gatto** | Cat |
| **Mucca** | Cow |
| **Pecora** | Sheep |
| **Ragno** | Spider |
| **Scoiattolo** | Squirrel |

---
<img width="1582" height="695" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2025-12-07 203746" src="https://github.com/user-attachments/assets/abaaa52d-ac32-4ad0-962b-9b9ba13486e6" />

## ğŸ› ï¸ Techniques & Methodology
To achieve high performance and prevent overfitting on a custom architecture with **~25.8 Million parameters**, the following engineering decisions were made:

### 1. Data Augmentation (The "Noise" Factor)
* **Technique:** Random flips, rotations, and zooms were applied to the training images dynamically.
* **Reason:** Since we built a large model from scratch, there was a high risk of the model memorizing specific pixels. Augmentation introduces "noise" and variance, forcing the model to learn invariant features (e.g., recognizing a cat regardless of its orientation) rather than memorizing the exact image.

### 2. Custom CNN Architecture
* **Technique:** A sequential stack of 3 Convolutional Blocks (Filters: 32 -> 64 -> 128) followed by a massive Dense layer (256 units).
* **Reason:** The hierarchical structure allows the model to learn simple features (edges) in early layers and complex shapes (ears, eyes) in deeper layers.

### 3. Dropout Regularization
* **Technique:** A `Dropout(0.3)` layer was added before the final output.
* **Reason:** With over 25 million parameters in the dense layer, the model is prone to overfitting. Dropout randomly disables 30% of the neurons during training, preventing the network from relying too heavily on any single feature path.

### 4. Early Stopping & Model Checkpointing
* **Technique:** We monitored `val_accuracy` and saved only the best model.
* **Reason:** Training for too many epochs often leads to performance degradation on new data (overfitting). We restored the weights from **Epoch 22**, where the validation accuracy peaked, ensuring optimal generalization.

---

## ğŸ“Š Results & Evaluation

### Performance Metrics
The model achieved a solid baseline performance given it was trained from scratch:
* **Training Accuracy:** ~77.20%
* **Validation Accuracy:** ~72.02%
<img width="1316" height="536" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2025-12-07 191024" src="https://github.com/user-attachments/assets/728b234b-7da6-48c1-8ac8-25ce4d93070c" />

The ~5% gap between training and validation scores indicates that our regularization strategies (Augmentation & Dropout) successfully kept overfitting under control.

### Inference Analysis (Case Study: Spider)
Below is a prediction example on an unseen image of a **Spider (Ragno)**:
<img width="1184" height="864" alt="cnnspider" src="https://github.com/user-attachments/assets/5ed33174-9f7c-4350-9afa-7746c5a5f6c8" />


* **Outcome:** The model **correctly** classified the image as "Ragno".
* **Observation:** The 74.05% score indicates that the model is quite confident and reliable in its prediction for a 10-class problem. This level of confidence proves that the model has successfully learned the fundamental features. The remaining uncertainty (25.95%) stems from background noise or similarities to other classes in the visual data, and can be addressed through hyperparameter optimization to achieve confidence levels above 90%.
---
---
# ğŸ¾ Animals-10 GÃ¶rÃ¼ntÃ¼ SÄ±nÄ±flandÄ±rma

## ğŸ¯ Proje AmacÄ±
Bu projenin temel amacÄ±, 10 farklÄ± hayvan tÃ¼rÃ¼nÃ¼ sÄ±nÄ±flandÄ±rabilen gÃ¼Ã§lÃ¼ bir Derin Ã–ÄŸrenme modeli geliÅŸtirmektir. Transfer Ã–ÄŸrenme (Transfer Learning) yerine, sÄ±fÄ±rdan bir **Ã–zel EvriÅŸimli Sinir AÄŸÄ± (CNN)** mimarisi kurularak modelin genelleme yeteneÄŸi test edilmiÅŸtir.

---

## ğŸ“‚ Veri Seti DetaylarÄ±
Model, **Animals-10** veri seti Ã¼zerinde eÄŸitilmiÅŸtir. Veri seti toplam **26.179 gÃ¶rÃ¼ntÃ¼** iÃ§ermektedir.

* **EÄŸitim Seti:** 20.944 gÃ¶rÃ¼ntÃ¼
* **DoÄŸrulama (Validation) Seti:** 5.235 gÃ¶rÃ¼ntÃ¼
* **SÄ±nÄ±flar:** Veri seti, orijinal etiketleri Ä°talyanca olan 10 sÄ±nÄ±fÄ± kapsamaktadÄ±r:

| SÄ±nÄ±f AdÄ± | TÃ¼rkÃ§e KarÅŸÄ±lÄ±ÄŸÄ± |
|:---:|:---:|
| **Cane** | KÃ¶pek |
| **Cavallo** | At |
| **Elefante** | Fil |
| **Farfalla** | Kelebek |
| **Gallina** | Tavuk |
| **Gatto** | Kedi |
| **Mucca** | Ä°nek |
| **Pecora** | Koyun |
| **Ragno** | Ã–rÃ¼mcek |
| **Scoiattolo** | Sincap |

---

## ğŸ› ï¸ Teknikler ve Metodoloji
YaklaÅŸÄ±k **25.8 Milyon parametreye** sahip Ã¶zel mimaride aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) Ã¶nlemek ve performansÄ± artÄ±rmak iÃ§in ÅŸu mÃ¼hendislik kararlarÄ± alÄ±nmÄ±ÅŸtÄ±r:

### 1. Veri Ã‡oÄŸaltma (Data Augmentation - "GÃ¼rÃ¼ltÃ¼" FaktÃ¶rÃ¼)
* **Teknik:** EÄŸitim gÃ¶rÃ¼ntÃ¼leri Ã¼zerinde dinamik olarak rastgele Ã§evirme, dÃ¶ndÃ¼rme ve yakÄ±nlaÅŸtÄ±rma teknikleri uygulanmÄ±ÅŸtÄ±r.
* **GerekÃ§e:** SÄ±fÄ±rdan oluÅŸturulan bÃ¼yÃ¼k bir modelde, ezberleme riskini azaltmak iÃ§in bu teknik kullanÄ±lmÄ±ÅŸtÄ±r. Ã‡oÄŸaltma, eÄŸitime **varyasyon (gÃ¼rÃ¼ltÃ¼)** ekleyerek modelin gÃ¶rÃ¼ntÃ¼leri ezberlemek yerine, yÃ¶nelimden baÄŸÄ±msÄ±z (invariant) Ã¶zellikleri Ã¶ÄŸrenmeye zorlamÄ±ÅŸtÄ±r.

### 2. Ã–zel CNN Mimarisi
* **Teknik:** ÃœÃ§ EvriÅŸim BloÄŸu (Filtreler: 32 -> 64 -> 128) ve ardÄ±ndan 256 Ã¼niteden oluÅŸan yoÄŸun bir Tam BaÄŸlantÄ±lÄ± (Dense) katmandan oluÅŸan sÄ±ralÄ± (sequential) yÄ±ÄŸÄ±n kullanÄ±lmÄ±ÅŸtÄ±r.
* **GerekÃ§e:** HiyerarÅŸik yapÄ±, modelin erken katmanlarda basit kenar Ã¶zelliklerini, derin katmanlarda ise karmaÅŸÄ±k ÅŸekilleri (gÃ¶z, kulak gibi) Ã¶ÄŸrenmesine olanak tanÄ±mÄ±ÅŸtÄ±r.

### 3. Dropout (Seyreltme) DÃ¼zenlemesi
* **Teknik:** Son Ã§Ä±ktÄ±dan Ã¶nce bir `Dropout(0.3)` katmanÄ± eklenmiÅŸtir.
* **GerekÃ§e:** Tam baÄŸlantÄ±lÄ± katmandaki 25 Milyonun Ã¼zerindeki parametre nedeniyle aÅŸÄ±rÄ± Ã¶ÄŸrenme eÄŸilimi yÃ¼ksektir. Dropout, eÄŸitim sÄ±rasÄ±nda nÃ¶ronlarÄ±n %30'unu rastgele devre dÄ±ÅŸÄ± bÄ±rakarak modelin tek bir Ã¶zelliÄŸe aÅŸÄ±rÄ± baÄŸÄ±mlÄ± olmasÄ±nÄ± Ã¶nlemiÅŸtir.

### 4. Erken Durdurma ve Model Kaydetme
* **Teknik:** `val_accuracy` deÄŸeri izlenmiÅŸ ve yalnÄ±zca en iyi performansÄ± gÃ¶steren modelin aÄŸÄ±rlÄ±klarÄ± kaydedilmiÅŸtir.
* **GerekÃ§e:** Uzun sÃ¼reli eÄŸitimin performans dÃ¼ÅŸÃ¼ÅŸÃ¼ne yol aÃ§mamasÄ± iÃ§in, doÄŸrulama baÅŸarÄ±sÄ±nÄ±n zirveye ulaÅŸtÄ±ÄŸÄ± **22. Epoch**'taki aÄŸÄ±rlÄ±klar restore edilerek optimum genelleme saÄŸlanmÄ±ÅŸtÄ±r.

---

## ğŸ“Š SonuÃ§lar ve DeÄŸerlendirme

### Performans Metrikleri
SÄ±fÄ±rdan eÄŸitilen bir modele gÃ¶re gÃ¼Ã§lÃ¼ bir baÅŸlangÄ±Ã§ performansÄ± elde edilmiÅŸtir:
* **EÄŸitim DoÄŸruluÄŸu (Training Accuracy):** ~%77.20
* **DoÄŸrulama DoÄŸruluÄŸu (Validation Accuracy):** ~%72.02

EÄŸitim ve doÄŸrulama skorlarÄ± arasÄ±ndaki yaklaÅŸÄ±k %5'lik fark, uygulanan dÃ¼zenleme stratejilerinin (Ã‡oÄŸaltma ve Dropout) aÅŸÄ±rÄ± Ã¶ÄŸrenmeyi baÅŸarÄ±lÄ± bir ÅŸekilde kontrol altÄ±nda tuttuÄŸunu gÃ¶stermektedir.

### Ã‡Ä±karÄ±m Analizi (Ã–rnek Durum: Ã–rÃ¼mcek)

* **SonuÃ§:** Model, gÃ¶rÃ¼nmeyen bir test gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ **"Ragno" (Ã–rÃ¼mcek)** olarak doÄŸru sÄ±nÄ±flandÄ±rmÄ±ÅŸtÄ±r.
* **GÃ¶zlem (Observation):** GÃ¼ven skoru **%74.05** olarak kaydedilmiÅŸtir. Bu oran, 10 sÄ±nÄ±flÄ± bir problem iÃ§in modelin tahmininde **oldukÃ§a emin ve gÃ¼venilir** olduÄŸunu gÃ¶sterir. Bu gÃ¼ven dÃ¼zeyi, modelin temel Ã¶zellikleri baÅŸarÄ±yla Ã¶ÄŸrendiÄŸini kanÄ±tlar. Kalan belirsizlik (%25.95) ise, gÃ¶rseldeki arka plan karmaÅŸasÄ± veya sÄ±nÄ±f benzerliklerinden kaynaklanmaktadÄ±r ve %90 Ã¼zeri gÃ¼ven iÃ§in **hiperparametre optimizasyonu** ile giderilebilir.
